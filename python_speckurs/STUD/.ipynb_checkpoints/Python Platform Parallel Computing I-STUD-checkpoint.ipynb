{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img src=\"Images/SKI_Header_1_Tr.png\" width=\"150%\"/></div>\n",
    "<p style=\"text-align: center; font-size: 36pt; font-weight: bold; color: rgb(0, 0, 190); padding: 20px; margin-top: 2%;\">СПЕЦКУРС</p>\n",
    "<p style=\"text-align: center; font-size: 36pt; font-weight: bold; color: rgb(0, 0, 190); padding: 40px; margin-top: -3%; line-height: 100%;\">Высокопроизводительные вычисления на платформе Python</p>\n",
    "<p style=\"text-align: left; font-size: 20pt; font-weight: bold; color: rgb(0, 0, 190); padding: 20px; margin-top: -2%; line-height: 10%;\">Преподаватель:</p>\n",
    "<p style=\"text-align: left; font-size: 20pt; font-weight: bold; color: rgb(0, 0, 190); padding: 18px; margin-top: -2%; line-height: 100%;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Русол Андрей Владимирович, к.т.н., с.н.с. ГЕОХИ РАН</p>\n",
    "<p style=\"text-align: center; font-size: 16pt; font-weight: bold; color: rgb(0, 0, 190); padding: 20px; margin-top: 2%;\">Москва 2021</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"text-align: center; font-size: 36pt; font-weight: bold; color: rgb(0, 0, 190); padding: 40px; margin-top: 3%; line-height: 150%;\">Технологии параллельного программирования на платформе Python: пакеты IPyparallel, MPI4Py и др.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "С появлением многоядерных процессоров параллельная реализация алгоритмов стала общеупотребительной практикой прикладного программирования, распространяющей нагрузку на все доступные ядра. В настоящее время сформировались четыре основных архитектуры параллельного программирования:\n",
    "- Single instruction, single data (SISD)\n",
    "- Single instruction, multiple data (SIMD)\n",
    "- Multiple instruction, single data (MISD)\n",
    "- Multiple instruction, multiple data (MIMD)\n",
    "\n",
    "#### Архитектура **SISD**\n",
    "\n",
    "Вычислительная система в архитектуре SISD является однопроцессорной машиной. Она выполняет одну команду, которая работает с одним потоком данных. В SISD машинные инструкции обрабатываются последовательно.\n",
    "\n",
    "#### Архитектура **МISD**\n",
    "\n",
    "В этой модели ***N*** процессоров, каждый со своим блоком управления, совместно используют **ОДИН** блок памяти. В каждом такте данные, полученные из памяти, обрабатываются всеми процессорами одновременно, каждый в соответствии с инструкциями, полученными от его блока управления. В этом случае параллелизм (***параллелизм на уровне инструкций***) получается путем выполнения нескольких операций над одним и тем же фрагментом данных. Типы задач, которые могут быть эффективно решены в этой архитектуре, довольно специфичны, например, шифрование данных; по этой причине архитектура **MISD** удел скорее специализированного оборудования, чем широко применяемая практическая конфигурация вычислительных систем.\n",
    "\n",
    "#### Архитектура **SIMD**\n",
    "\n",
    "**SIMD**-компьютер состоит из ***N*** одинаковых процессоров, каждый со своей собственной локальной памятью. Все процессоры работают под управлением одного потока команд; в дополнение к этому, есть ***N*** потоков данных, по одному для каждого процессора. Процессоры работают ***одновременно*** на каждом шаге и выполняют ***одну*** и ту же инструкцию, но ***на разных*** элементах данных. Это пример параллелизма на уровне данных. Архитектуры **SIMD** гораздо более универсальны, чем архитектуры **MISD**. C помощью параллельных алгоритмов на **SIMD**-компьютерах могут быть решены mногочисленные задачи, охватывающие широкий спектр приложений. Еще одна интересная особенность заключается в том, что алгоритмы для этих компьютеров относительно просты в проектировании, анализе и реализации. Ограничение состоит в том, что на **SIMD**-архитектуре могут быть решены только задачи, которые могут быть разделены на несколько одинаковых подзадач, которые будут решаться одновременно, одним и тем же набором инструкций. Современные **GPU**-вычислители построены с использованием встроенных модулей **SIMD**, что привело к широкому использованию этой вычислительной парадигмы.\n",
    "\n",
    "#### Архитектура **МIMD**\n",
    "\n",
    "Этот класс параллельных архитектур является самым общим и более мощным классом в соответствии с классификацией Флинна. В архитектуре **МIMD** реализовано ***N*** процессоров, ***N*** потоков команд и ***N*** потоков данных. Каждый процессор имеет свой собственный блок управления и локальную память, что делает архитектуры **MIMD** более мощными в вычислительном отношении, чем те, которые используются в **SIMD**. ***Каждый процессор*** работает под управлением ***потока инструкций***, выдаваемых его ***собственным блоком управления***; поэтому процессоры могут потенциально запускать ***разные программы на разных данных***, решая различные подзадачи, которые могут быть частью одной большой задачи. Высокая производительность в **MIMD**-архитектуре достигается с помощью уровня параллелизма с **потоками** и/или **процессами**. Это также означает, что процессоры обычно работают ***асинхронно***. В настоящее время эта архитектура применяется во многих компьютерах, суперкомпьютерах и вычислительных компьютерных сетях. Однако нужно учитывать, что асинхронные алгоритмы сложно проектировать, анализировать и реализовывать.\n",
    "\n",
    "#### Современные гетерогенные архитектуры\n",
    "\n",
    "Появление **GPU**-вычислителей изменило характер того, как строятся, используются и программируются высокопроизводительные вычислительные системы и суперкомпьютеры. Несмотря на высокую производительность, предлагаемую графическими процессорами, они не могут считаться автономным процессором, так как они всегда должны сопровождаться обычными процессорами и блоками памяти. Поэтому парадигма программирования выглядит так:\n",
    "***центральный процессор*** управляет основным потоком программы ***последовательным образом***, передавая ***графическому ускорителю*** задачи, которые являются вычислительно ***дорогостоящими*** и имеют ***высокую степень параллелизма***. Связь между процессором и графическим процессором может происходить не только за счет использования высокоскоростной шины, но и путем совместного использования одной области памяти. Фактически, в случае, когда оба устройства не имеют собственных областей памяти, можно обратиться к общей области памяти, используя библиотеки программного обеспечения, предоставляемые различными моделями программирования, такими как **CUDA** и **OpenCL**. Эти архитектуры называются ***гетерогенными архитектурами***, в которых приложения могут создавать структуры данных в одном адресном пространстве и отправлять задание на аппаратное обеспечение **GPU**-устройства. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"text-align: center; font-size: 48pt; font-weight: bold; color: rgb(0, 0, 190); padding: 4px; margin-top: 3%; line-height: 150%;\">Введение в технологии параллельного программирования</p>\n",
    "<p style=\"text-align: center; font-size: 72pt; font-weight: bold; color: rgb(0, 0, 190); padding: 10px; margin-top: 3%; line-height: 150%;\">OpenMP <font style=\"font-size: 48pt; font-weight: bold; color: rgb(0, 0, 190); line-height: 150%;\">и</font> MPI</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### <p style=\"text-align: center; font-size: 36pt; font-weight: bold; color: rgb(0, 0, 190); padding: 40px; margin-top: 3%; line-height: 150%;\">Параллельное программирование систем с общей памятью средствами OpenMP</p>\n",
    "\n",
    "Технология **OpenMP** является одним из наиболее популярных средств параллельного программирования для вычислительных систем с общей памятью. Концептуально технология **OpenMP** основана на использовании традиционных последовательных языков программирования, таких как **FORTRAN** и **С/С++**, и специализированных директив компилятора, управляющих библиотеками параллельной реализации алгоритмов. \n",
    "\n",
    "Для создания параллельной версии, из программы последовательной реализации вычислительного алгоритма, программисту предоставляется набор директив, функций и переменных окружения. Такой подход позволяет разрабатывать один и тот же вариант программы как для параллельного, так и последовательного исполнения.\n",
    "\n",
    "В технологии **OpenMP** параллелизм реализуется на основе ***многопоточности***. При запуске программы создается один управляющий ***\\\"master\\\"***-поток, который сам создает набор исполнительных ***\\\"slave\\\"***-потоков. Вычислительная нагрузка перераспределяется между всеми потоками, причем созданные потоки могут конкурировать между собой за процессорные ресурсы.\n",
    "\n",
    "Существенным достоинством технологии **OpenMP** является возможность поэтапного распараллеливания больших программ, помере нахождения участков с потенциальным параллелизмом. Это значительно облегчает процесс адаптации последовательных программ к параллельным вычислительным системам, а также отладку и оптимизацию программ. Программы могут содержать любое количество параллельных и последовательных участков, причем параллельные участки могут быть вложенными друг в друга.\n",
    "\n",
    "Активным стандартом **OpenMP** является стандарт **5.1**, принятый в 2015 году. \n",
    "\n",
    "Программа, использующая технологию **OpenMP**, может содержать следующие элементы:\n",
    "- ***директивы компилятора*** - создают потоки, распределяют нагрузку между потоками и обеспечивают их синхронизацию\n",
    "- ***подпрограммы библиотеки времени выполнения*** - используются для установки и обработки атрибутов потоков\n",
    "- ***переменные окружения*** - позволяют управлять поведением параллельных программ\n",
    "\n",
    "\n",
    "### Переменные\n",
    "\n",
    "В **OpenMP** переменные в параллельных областях программы разделяются на два основных класса:\n",
    "- ***private*** (локальные, приватные; каждая нить видит свой экземпляр данной переменной).\n",
    "- ***shared*** (общие; все нити видят одну и ту же переменную);\n",
    "\n",
    "По умолчанию, все переменные, порождённые ***вне*** параллельной области, ***при входе*** в неё остаются ***общими***. Исключение составляют переменные, являющиеся счетчиками итераций в цикле. Переменные, порождённые ***внутри*** параллельной области, по умолчанию являются ***локальными***.\n",
    "\n",
    "### Директивы\n",
    "\n",
    "Директивы **OpenMP** в языке **С/С++** задаются указаниями препроцессору, начинающимися с \n",
    "\n",
    "***#pragma omp***\n",
    "\n",
    "***#pragma omp directive-name \n",
    "[опция[[,] опция]...]***\n",
    "\n",
    "Объектом действия большинства директив является один оператор или блок, перед которым расположена директива в исходном тексте программы.\n",
    "\n",
    "Чтобы задействовать функции библиотеки **OpenMP** периода выполнения (исполняющей среды), в программу нужно включить заголовочный файл ***omp.h*** (для программ на языке **FORTRAN** – файл ***omp_lib.h*** или модуль ***omp_lib***).\n",
    "\n",
    "Также нужно задать количество нитей, выполняющих параллельные области программы, определив значение переменной среды **OMP_NUM_THREADS**:\n",
    "\n",
    "**export OMP_NUM_THREADS=n**\n",
    "\n",
    "### Параллельные и последовательные области\n",
    "\n",
    "Параллельные области задаются директивой\n",
    "\n",
    "**#pragma omp parallel [опция[[,] опция]...]**\n",
    "\n",
    "Данная директива порождает **OMP_NUM_THREADS-1** нитей, каждая нить получает свой уникальный номер, причём порождающая нить получает номер 0 и становится основной нитью группы (**«master»**-нитью). При выходе из параллельной области производится неявная синхронизация и уничтожаются все нити, кроме породившей.\n",
    "\n",
    "### Пример № 1: Параллельная область \n",
    "```c++\n",
    "#include <stdio.h>\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "printf(\"Последовательная область 1\\n\");\n",
    "#pragma omp parallel\n",
    "{\n",
    "printf(\"Параллельная область\\n\");\n",
    "}\n",
    "printf(\"Последовательная область 2\\n\");\n",
    "}\n",
    "```\n",
    "Сохраним этот код в файл ***OMP_Hello.cpp***, скомпилируем и выполним\n",
    "\n",
    "#### На локальной машине\n",
    "Компиляцию этой программы проводим командой\n",
    "```bash\n",
    "g++ -fopenmp <File Name>.cрр -o <EXEC>\n",
    "```\n",
    "После компиляции в терминале вводим команду\n",
    "```bash\n",
    "export OMP_NUM_THREADS=12\n",
    "```\n",
    "Затем в терминале вводим команду\n",
    "```bash\n",
    "./<EXEC>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T14:08:04.252518Z",
     "start_time": "2021-04-02T14:08:04.243159Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile WORK_SPACE/OpenMP/OMP_Hello.cpp\n",
    "#include <stdio.h>\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "printf(\"Последовательная область 1\\n\");\n",
    "#pragma omp parallel\n",
    "{\n",
    "printf(\"Параллельная область\\n\");\n",
    "}\n",
    "printf(\"Последовательная область 2\\n\");\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T14:08:34.390439Z",
     "start_time": "2021-04-02T14:08:34.268519Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls WORK_SPACE/OpenMP/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T14:09:37.654317Z",
     "start_time": "2021-04-02T14:09:37.306695Z"
    }
   },
   "outputs": [],
   "source": [
    "!g++ -fopenmp WORK_SPACE/OpenMP/OMP_Hello.cpp -o WORK_SPACE/OpenMP/omp_hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T14:09:38.714729Z",
     "start_time": "2021-04-02T14:09:38.573822Z"
    }
   },
   "outputs": [],
   "source": [
    "!WORK_SPACE/OpenMP/omp_hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T14:10:54.700336Z",
     "start_time": "2021-04-02T14:10:54.555280Z"
    }
   },
   "outputs": [],
   "source": [
    "!export OMP_NUM_THREADS=12; WORK_SPACE/OpenMP/omp_hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Функции OpenMP для работы с временем\n",
    "Функции для работы с системным таймером:\n",
    "```c++\n",
    "double omp_get_wtime(void);\n",
    "```\n",
    "Возвращает астрономическое время в секундах, прошедшее с некоторого момента в прошлом.\n",
    "```c++\n",
    "double omp_get_wtick(void);\n",
    "```\n",
    "Возвращает в вызвавшей нити разрешение таймера в секундах.\n",
    "\n",
    "### Некоторые функции OpenMP для параллельной работы\n",
    "\n",
    "**reduction(оператор:список)** – задаёт оператор и список общих переменных. Для каждой переменной создаются локальные копии в каждой нити и инициализируются соответственно типу оператора (для аддитивных – 0 или аналоги, для мультипликативных – 1 или аналоги). После выполнения всех операторов параллельной области выполняется заданный **оператор**; **оператор** - это (для языка **С/С++**):  **+, *, -, &, |, ^, &&, ||**. Порядок выполнения операторов не определён, поэтому результат может отличаться от запуска к запуску.\n",
    "\n",
    "**omp_in_parallel()** – возвращает 1, если она была вызвана из активной параллельной области программы.\n",
    "\n",
    "### Синхронизация в OpenMP\n",
    "\n",
    "Самый распространенный способ синхронизации в **OpenMP** – ***барьер***. Он оформляется с помощью директивы **barrier**.\n",
    "```c++\n",
    "#pragma omp barrier\n",
    "```\n",
    "Нити, выполняющие текущую параллельную область, дойдя до этой директивы, останавливаются и ждут, пока все нити не дойдут до этой точки программы, после чего разблокируются и продолжают работать дальше. Кроме того, для ***разблокировки*** необходимо, чтобы ***все*** синхронизируемые ***нити*** завершили ***все*** порождённые ими ***задачи (task)***.\n",
    "\n",
    "### Пример №2: Перемножение двух квадратных матриц\n",
    "\n",
    "```c++\n",
    "#include <stdio.h>\n",
    "#include <omp.h>\n",
    "#define N 1024\n",
    "#define NTeams 32\n",
    "#define NThreads N/NTeams\n",
    "\n",
    "\n",
    "double a[N][N], b[N][N], c[N][N];\n",
    "\n",
    "int main()\n",
    "{\n",
    " int i, j, k;\n",
    " double t1, t2, t3, t4, t5, t6, t7, t8, t9, t10, t11, t12, t13, t14, t15, t16;\n",
    " // инициализация матриц\n",
    " for (i=0; i<N; i++){\n",
    "     for (j=0; j<N; j++){\n",
    "         a[i][j]=b[i][j]=i*j;\n",
    "         c[i][j]=0.0;\n",
    "     }\n",
    " }\n",
    "\n",
    " t1=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " for(i=0; i<N; i++){\n",
    "          for(j=0; j<N; j++){\n",
    "                    for(k=0; k<N; k++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " t2=omp_get_wtime();\n",
    " printf(\"Serial IJK Time= %lf sec\\n\", t2-t1);\n",
    "\n",
    " t3=omp_get_wtime();\n",
    " // основной вычислительный блок KIJ\n",
    " for(k=0; k<N; k++){\n",
    "          for(i=0; i<N; i++){\n",
    "                   for(j=0; j<N; j++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " t4=omp_get_wtime();\n",
    " printf(\"Serial KIJ Time= %lf sec\\n\", t4-t3);\n",
    "\n",
    " t5=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " #pragma omp parallel for shared(a, b, c) private(i, j, k)\n",
    " for(i=0; i<N; i++){\n",
    "          for(j=0; j<N; j++){\n",
    "                   for(k=0; k<N; k++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " t6=omp_get_wtime();\n",
    " printf(\"OpenMP IJK Time= %lf sec\\n\", t6-t5);\n",
    "\n",
    " t7=omp_get_wtime();\n",
    " // основной вычислительный блок KIJ\n",
    " #pragma omp parallel for shared(a, b, c) private(i, j, k)\n",
    " for(k=0; k<N; k++){\n",
    "          for(i=0; i<N; i++){\n",
    "                   for(j=0; j<N; j++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " t8=omp_get_wtime();\n",
    " printf(\"OpenMP KIJ Time= %lf sec\\n\", t8-t7);\n",
    "\n",
    " t9=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " #pragma omp target map(to: a,b) map(from: c)\n",
    " #pragma omp parallel for private(i,j,k)\n",
    " for(i=0; i<N; i++){\n",
    "          for(j=0; j<N; j++){\n",
    "                   for(k=0; k<N; k++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " t10=omp_get_wtime();\n",
    " printf(\"OpenMP Naive GPU IJK Time= %lf sec\\n\", t10-t9);\n",
    "\n",
    " t11=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " #pragma omp target map(to: a,b) map(from: c)\n",
    " #pragma omp parallel for private(i,j)\n",
    " for(k=0; k<N; k++){\n",
    "          for(i=0; i<N; i++){\n",
    "                   for(j=0; j<N; j++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    "\n",
    " t12=omp_get_wtime();\n",
    " printf(\"OpenMP Naive GPU KIJ Time= %lf sec\\n\", t12-t11);\n",
    "\n",
    " t13=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " #pragma omp target data map(to: a,b) map(tofrom: c)\n",
    " {\n",
    " #pragma omp target teams num_teams(NTeams) thread_limit(NThreads)\n",
    " #pragma omp parallel for private(j,k)\n",
    " for(i=0; i<N; i++){\n",
    "          for(j=0; j<N; j++){\n",
    "                   for(k=0; k<N; k++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " }\n",
    " t14=omp_get_wtime();\n",
    " printf(\"OpenMP Teams GPU IJK Time= %lf sec\\n\", t14-t13);\n",
    "\n",
    " t15=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " #pragma omp target data map(to: a,b) map(tofrom: c)\n",
    " {\n",
    " #pragma omp target teams num_teams(NTeams) thread_limit(NThreads)\n",
    " #pragma omp parallel for private(i,j)\n",
    " for(k=0; k<N; k++){\n",
    "          for(i=0; i<N; i++){\n",
    "                   for(j=0; j<N; j++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " }\n",
    " t16=omp_get_wtime();\n",
    " printf(\"OpenMP Teams GPU KIJ Time= %lf sec\\n\", t16-t15);\n",
    "\n",
    "}\n",
    "```\n",
    "g++ -fopenmp -omptargets=nvptx64sm_35-nvidia-linux OMP_MatMul.cpp -o omp_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T14:33:11.824180Z",
     "start_time": "2021-04-02T14:33:11.795088Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile WORK_SPACE/OpenMP/OMP_MatMul.cpp\n",
    "#include <stdio.h>\n",
    "#include <omp.h>\n",
    "#define N 1024\n",
    "#define NTeams 32\n",
    "#define NThreads N/NTeams\n",
    "\n",
    "\n",
    "double a[N][N], b[N][N], c[N][N];\n",
    "\n",
    "int main()\n",
    "{\n",
    " int i, j, k;\n",
    " double t1, t2, t3, t4, t5, t6, t7, t8, t9, t10, t11, t12, t13, t14, t15, t16;\n",
    " // инициализация матриц\n",
    " for (i=0; i<N; i++){\n",
    "     for (j=0; j<N; j++){\n",
    "         a[i][j]=b[i][j]=i*j;\n",
    "         c[i][j]=0.0;\n",
    "     }\n",
    " }\n",
    "\n",
    " t1=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " for(i=0; i<N; i++){\n",
    "          for(j=0; j<N; j++){\n",
    "                    for(k=0; k<N; k++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " t2=omp_get_wtime();\n",
    " printf(\"Serial IJK Time= %lf sec\\n\", t2-t1);\n",
    "\n",
    " t3=omp_get_wtime();\n",
    " // основной вычислительный блок KIJ\n",
    " for(k=0; k<N; k++){\n",
    "          for(i=0; i<N; i++){\n",
    "                   for(j=0; j<N; j++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " t4=omp_get_wtime();\n",
    " printf(\"Serial KIJ Time= %lf sec\\n\", t4-t3);\n",
    "\n",
    " t5=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " #pragma omp parallel for shared(a, b, c) private(i, j, k)\n",
    " for(i=0; i<N; i++){\n",
    "          for(j=0; j<N; j++){\n",
    "                   for(k=0; k<N; k++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " t6=omp_get_wtime();\n",
    " printf(\"OpenMP IJK Time= %lf sec\\n\", t6-t5);\n",
    "\n",
    " t7=omp_get_wtime();\n",
    " // основной вычислительный блок KIJ\n",
    " #pragma omp parallel for shared(a, b, c) private(i, j, k)\n",
    " for(k=0; k<N; k++){\n",
    "          for(i=0; i<N; i++){\n",
    "                   for(j=0; j<N; j++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " t8=omp_get_wtime();\n",
    " printf(\"OpenMP KIJ Time= %lf sec\\n\", t8-t7);\n",
    "\n",
    " t9=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " #pragma omp target map(to: a,b) map(from: c)\n",
    " #pragma omp parallel for private(i,j,k)\n",
    " for(i=0; i<N; i++){\n",
    "          for(j=0; j<N; j++){\n",
    "                   for(k=0; k<N; k++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " t10=omp_get_wtime();\n",
    " printf(\"OpenMP Naive GPU IJK Time= %lf sec\\n\", t10-t9);\n",
    "\n",
    " t11=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " #pragma omp target map(to: a,b) map(from: c)\n",
    " #pragma omp parallel for private(i,j)\n",
    " for(k=0; k<N; k++){\n",
    "          for(i=0; i<N; i++){\n",
    "                   for(j=0; j<N; j++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    "\n",
    " t12=omp_get_wtime();\n",
    " printf(\"OpenMP Naive GPU KIJ Time= %lf sec\\n\", t12-t11);\n",
    "\n",
    " t13=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " #pragma omp target data map(to: a,b) map(tofrom: c)\n",
    " {\n",
    " #pragma omp target teams num_teams(NTeams) thread_limit(NThreads)\n",
    " #pragma omp parallel for private(j,k)\n",
    " for(i=0; i<N; i++){\n",
    "          for(j=0; j<N; j++){\n",
    "                   for(k=0; k<N; k++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " }\n",
    " t14=omp_get_wtime();\n",
    " printf(\"OpenMP Teams GPU IJK Time= %lf sec\\n\", t14-t13);\n",
    "\n",
    " t15=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " #pragma omp target data map(to: a,b) map(tofrom: c)\n",
    " {\n",
    " #pragma omp target teams num_teams(NTeams) thread_limit(NThreads)\n",
    " #pragma omp parallel for private(i,j)\n",
    " for(k=0; k<N; k++){\n",
    "          for(i=0; i<N; i++){\n",
    "                   for(j=0; j<N; j++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " }\n",
    " t16=omp_get_wtime();\n",
    " printf(\"OpenMP Teams GPU KIJ Time= %lf sec\\n\", t16-t15);\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!g++ -fopenmp -omptargets=nvptx64sm_75-nvidia-linux  WORK_SPACE/OpenMP/OMP_MatMul.cpp -o WORK_SPACE/OpenMP/omp_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T14:34:27.738144Z",
     "start_time": "2021-04-02T14:34:27.445106Z"
    }
   },
   "outputs": [],
   "source": [
    "!g++ -fopenmp -omptargets=nvptx64sm_35-nvidia-cuda -lcudart -L/usr/local/cuda/lib64 WORK_SPACE/OpenMP/OMP_MatMul.cpp -o WORK_SPACE/OpenMP/omp_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T14:36:38.428437Z",
     "start_time": "2021-04-02T14:34:32.966712Z"
    }
   },
   "outputs": [],
   "source": [
    "!WORK_SPACE/OpenMP/omp_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p style=\"text-align: center; font-size: 36pt; font-weight: bold; color: rgb(0, 0, 190); padding: 40px; margin-top: 3%; line-height: 150%;\">Параллельное программирование систем с распределенной памятью средствами MPI</p>\n",
    "\n",
    "***Message Passing Interface*** (**MPI**, интерфейс передачи сообщений) — стандарт программного интерфейса для передачи информации, который позволяет обмениваться сообщениями между процессами, выполняющими одну задачу.\n",
    "\n",
    "**MPI** является наиболее распространённым стандартом интерфейса обмена данными в параллельном программировании, существуют его реализации для большого числа компьютерных платформ. Используется при разработке программ для кластеров и суперкомпьютеров. Основным средством коммуникации между процессами в **MPI** является передача сообщений друг другу. \n",
    "\n",
    "### Параллелизм в MPI\n",
    "\n",
    "Цели распараллеливания:\n",
    "- равномерная загрузка вычислителей\n",
    "- минимизация количества и объёма необходимых пересылок данных\n",
    "\n",
    "При построении параллельного вычислительного процесса необходимо выделить группы операций, которые могут вычисляться независимо и одновременно. Возможность этого определяется наличием или отсутствием в программе истинных ***информационных зависимостей***. Две операции программы называются ***информационно зависимыми***, если ***результат*** выполнения ***одной*** операции используется в качестве ***аргумента*** в ***другой***. Пересылка данных требуется, если есть информационная зависимость между операциями, которые при выбранной схеме распределения попадают на разные вычислителей. Стандарт **MPI** 3.1 (4 июня 2015 года), содержит более 450 процедур. Стандартные функции начинаются с префикса **MPI_**.\n",
    "\n",
    "### Основные термины MPI\n",
    "\n",
    "- ***Процессы*** - обрабатывают данные и посылают сообщения\n",
    "- ***Сообщение*** – массив однотипных данных\n",
    "- ***Группа*** – упорядоченное множество процессов\n",
    "- ***Коммуникатор*** – контекст обмена группы\n",
    "\n",
    "В языке **С/С++** коммуникаторы имеют предопределённый тип **MPI_Comm**.\n",
    "\n",
    "##### Виды коммуникаторов\n",
    "\n",
    "- **MPI_COMM_WORLD** – коммуникатор для всех процессов приложения\n",
    "- **MPI_COMM_SELF** – коммуникатор, включающий только текущий процесс\n",
    "- **MPI_COMM_NULL** – коммуникатор, не содержащий ни одного процесса\n",
    "\n",
    "Каждый процесс может одновременно входить в разные коммуникаторы. Два основных атрибута процесса: **коммуникатор (группа)** и **номер процесса** в коммуникаторе (группе). Если коммуникатор содержит ***n*** процессов, то номера процессов в нём лежат в пределах от ***0*** до ***n–1***.\n",
    "\n",
    "- ***Сообщение*** — набор данных некоторого типа.\n",
    "\n",
    "Атрибуты сообщения: номер процесса-отправителя, номер процесса-получателя, идентификатор сообщения, коммуниктор.\n",
    "\n",
    "- ***Идентификатор сообщения (тег)*** - целое неотрицательное число в диапазоне от ***0*** до **MPI_TAG_UB**\n",
    "\n",
    "Для работы с атрибутами сообщений введена структура **MPI_Status**. \n",
    "\n",
    "### Некоторые процедуры MPI\n",
    "\n",
    "***Инициализация параллельной части*** программы.\n",
    "```c++\n",
    "int MPI_Init(int *argc, char ***argv)\n",
    "```\n",
    "Почти все другие процедуры **MPI** могут быть вызваны только после вызова **MPI_Init**. Инициализация параллельной части для каждого приложения должна выполняться только один раз.\n",
    "\n",
    "***Завершение параллельной части*** программы.\n",
    "```c++\n",
    "int MPI_Finalize(void)\n",
    "```\n",
    "Все последующие обращения к большинству процедур **MPI**, в том числе к **MPI_Init**, запрещены. К моменту вызова **MPI_Finalize** каждым процессом программы все действия, требующие его участия в обмене сообщениями, должны быть завершены. \n",
    "```c++\n",
    "int MPI_Initialized(int *flag)\n",
    "```\n",
    "В аргументе ***flag*** возвращает 1, если вызвана после процедуры **MPI_Init**, и 0 - в противном случае.\n",
    "```c++\n",
    "int MPI_Finalized(int *flag)\n",
    "```    \n",
    "В аргументе ***flag*** возвращает 1, если вызвана после процедуры MPI_Finalize , и 0 - в противном случае.\n",
    "Эти процедуры можно вызвать до **MPI_Init** и после **MPI_Finalize**.\n",
    "```c++\n",
    "int MPI_Comm_size(MPI_Comm comm, int *size)\n",
    "```\n",
    "В аргументе ***size*** возвращает число параллельных процессов в коммуникаторе comm.\n",
    "```c++\n",
    "int MPI_Comm_rank(MPI_Comm comm, int *rank)\n",
    "```\n",
    "В аргументе ***rank*** возвращает номер процесса в коммуникаторе comm в диапазоне от ***0*** до ***size-1***.\n",
    "```c++\n",
    "double MPI_Wtime(void)\n",
    "```\n",
    "Возвращает для каждого вызвавшего процесса астрономическое время в секундах (вещественное число двойной точности), прошедшее с некоторого момента в прошлом. Момент времени, используемый в качестве точки отсчёта, не будет изменён за время существования процесса.\n",
    "```c++\n",
    "double MPI_Wtick(void)\n",
    "```\n",
    "Возвращает разрешение таймера в секундах. Таймеры разных процессов могут быть не синхронизированы и выдавать существенно различающиеся значения, это можно определить по значению предопределенной константы **MPI_WTIME_IS_GLOBAL**: ***1*** – синхронизированы, ***0*** – нет.\n",
    "```c++\n",
    "int MPI_Get_processor_name(char *name, int *len)\n",
    "```\n",
    "Возвращает в строке name имя узла, на котором запущен вызвавший процесс. В переменной ***len*** возвращается количество символов в имени, не превышающее константы **MPI_MAX_PROCESSOR_NAME**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Параллельное программирование в Python \n",
    "\n",
    "Платформа **Python** включает в себя ряд механизмов, реализующих параллельные технологии, такие как **``threading``**, **``queues``**,**``multiprocessing``** и др. Долгое время модуль **``threading``** использовался как основной способ достижения параллельности. Некоторое время назад, модуль **``multiprocessing``** был добавлен в пакет стандартных библиотек **Python**. \n",
    "\n",
    "Процесс (**process**) - это исполняемый экземпляр приложения. Нить (**thread**) представляет собой активный поток управления, который может быть активирован параллельно с другими потоками в рамках одного и того же процесса. Каждый поток может выполнять набор инструкций (обычно, функцию) независимо и параллельно с другими процессами или потоками. Однако, будучи разными активными потоками в одном и том же процессе, они совместно используют адресацию и  структуры данных. Поток иногда называют легким процессом, поскольку он обладает многими характеристиками процесса, т.е. реализация потока менее обременительна, чем реализация реального процесса. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование модуля subprocess\n",
    "\n",
    "Модуль subprocess служит для создания и управления дочерними процессами и в современном Python является лучшим для такого рода задач. Запуск дочернего процесса осуществляется спомощью конструктора **Popen**. Метод **communicate** считывает выходные данные процесса и ожидает прекращения его выполнения. Дочерние процессы выполняются параллельно интерпретатору, что позволяет эффективнее использовать ресурсы системы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:13:02.632644Z",
     "start_time": "2021-04-02T15:13:02.624887Z"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:14:35.969638Z",
     "start_time": "2021-04-02T15:14:35.941154Z"
    }
   },
   "outputs": [],
   "source": [
    "proc = subprocess.Popen(['echo','Привет из дочернего процесса !!!'],stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:14:37.908004Z",
     "start_time": "2021-04-02T15:14:37.901951Z"
    }
   },
   "outputs": [],
   "source": [
    "out, err = proc.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:14:57.738795Z",
     "start_time": "2021-04-02T15:14:57.732512Z"
    }
   },
   "outputs": [],
   "source": [
    "print(out.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В дочернем процессе можно запускать команды системы, утилиты и программы, работающие в консоли. В Python 3.3 и выше реализован механизм, позволяющий при передаче методу **communicate** параметра **timeout** остановить дочерний процесс при превышении значения **timeout**. Это позволяет избежать блокировки системы при зависании дочернего процесса.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование модуля concurrent.futures\n",
    "\n",
    "Данный модуль позволяет использовать для параллельных вычислений несколько ядер CPU, запуская несколько интерпретаторов Python в виде дочерних процессов. В этом модуле, среди прочего, реализованы два класса:  \n",
    " - **ThreadPoolExecutor** - создания параллельных потоков;\n",
    " - **ProcessPoolExecutor** - для создания ппараллельных процессов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:19:09.830092Z",
     "start_time": "2021-04-02T15:19:09.801895Z"
    }
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import math\n",
    "\n",
    "WORKERS = 4\n",
    "\n",
    "PRIMES = [\n",
    "    112272535095293,\n",
    "    112582705942171,\n",
    "    112272535095293,\n",
    "    115280095190773,\n",
    "    115797848077099,\n",
    "    1099726899285419]\n",
    "\n",
    "def is_prime(n):\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "\n",
    "    sqrt_n = int(math.floor(math.sqrt(n)))\n",
    "    for i in range(3, sqrt_n + 1, 2):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def Threads():\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=WORKERS) as executor:\n",
    "        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):\n",
    "            print('%d is prime: %s' % (number, prime))\n",
    "\n",
    "def Process():\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=WORKERS) as executor:\n",
    "        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):\n",
    "            print('%d is prime: %s' % (number, prime))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Последовательная версия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:19:19.977516Z",
     "start_time": "2021-04-02T15:19:13.526515Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for number, prime in zip(PRIMES, map(is_prime, PRIMES)):\n",
    "    print('%d is prime: %s' % (number, prime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параллельная версия на потоках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:21:03.696116Z",
     "start_time": "2021-04-02T15:20:56.782795Z"
    }
   },
   "outputs": [],
   "source": [
    "%time Threads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параллельная версия на процессах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:21:37.430518Z",
     "start_time": "2021-04-02T15:21:33.086179Z"
    }
   },
   "outputs": [],
   "source": [
    "%time Process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из-за накладных расходов версия основанная на классе **ThreadPoolExecutor** работает медленнее последовательной. Версия основанная на классе **ProcessPoolExecutor** работает существенно производительнее. Это связано с тем, что класс **ProcessPoolExecutor** работает посредством низкоуровневых конструкций, предоставляемых модулем **multiprocessing**. Последовательность действий такая\n",
    "\n",
    " 1. Каждый элемент данных передается функции map()\n",
    " 2. Сериализует в двоичные данные с помощью модуля pickle\n",
    " 3. Сериализованные данные копируются из процесса основного интерпретатора в процессы дочерних\n",
    " 4. В дочернем процессе данные десериализуются в объекты Python с помощью модуля pickle\n",
    " 5. Выполняются вычисления в дочерних процессах\n",
    " 6. Сериализует результаты в двоичные данные \n",
    " 7. Копирует данные на родительский процесс\n",
    " 8. Десериализует данные в объекты Python\n",
    " 9. Обрабатывает данные, полученные от дочерних процессов\n",
    " \n",
    "Не смотря на большой объем скрытой от нас работы, класс **ProcessPoolExecutor** выполняет вычисления эффективнее. Такая схема идеально подходит для параллельного решения изолированных задач с незначительным объемом обмениваемых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p style=\"text-align: center; font-size: 36pt; font-weight: bold; color: rgb(0, 0, 190); padding: 40px; margin-top: 3%; line-height: 150%;\">Настройка работы <b>Jupyter</b> для использования параллельных технологий: пакеты IPyparallel, MPI4Py и др.</p>\n",
    "\n",
    "Пакет **IPyparallel** предоставляет набор инструментов для организации параллельных вычислительных процессов. Кластер организуемый по-умолчанию обеспечивает распераллеливание на основе технологий ***threading***, ***queues*** и ***multiprocessing***. Однако предеоставляются возможности организации вычислительных кластеров на основе других технологий параллельного исполнения вычислительных процессов.\n",
    "\n",
    "Для инициализации вкладки ***IPython Clusters*** в **Jupyter** необходимо выполнить команду\n",
    "```bash\n",
    "ipcluster nbextension enable [--user]\n",
    "```\n",
    "Для отключения\n",
    "```bash\n",
    "ipcluster nbextension disable [--user]\n",
    "```\n",
    "\n",
    "Для построения кластера на основе технологии **MPI** необходимо сгенерировать шаблон профиля, для этого в терминале выполняем команду\n",
    "```bash\n",
    "ipython profile create --parallel --profile=mpi\n",
    "```\n",
    "В папке с файлами профиля необходимо отредактироавть файл ***IPYTHONDIR/profile_mpi/ipcluster_config.py***, вставив\n",
    "в конец строку\n",
    "```bash\n",
    "c.IPClusterEngines.engine_launcher_class = 'MPIEngineSetLauncher'\n",
    "```\n",
    "Для запуска **MPI**-кластера  в терминале, выполняем команду\n",
    "```bash\n",
    "ipcluster start -n 4 --profile=mpi\n",
    "```\n",
    "\n",
    "Для начала использования организованного вычислительного кластера в ячейках **Jupyter**, необходимо импортировать пакет **IPyparallel**\n",
    "```python\n",
    "import ipyparallel\n",
    "```\n",
    "В следующей ячейке создаем экземпляр клиента и объекта доступа к свойствам и методам.\n",
    "- для кластера по-умолчанию\n",
    "```python\n",
    "client = ipyparallel.Client()\n",
    "dview = client[:]\n",
    "```\n",
    "- для **MPI**-кластера\n",
    "```python\n",
    "client = ipyparallel.Client(profile='mpi')\n",
    "dview = client[:]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример № 1 : Построение фрактального множества Жюлиа\n",
    "\n",
    "***Множество Жюлиа*** функции $\\;f$ обозначаемое $J\\left( f \\right)$ определяется как\n",
    "$$\n",
    "J\\left( f \\right)=\\partial \\left\\{ z:f^{(n)} \\rightarrow \\infty \\;\\; \\text{при} \\;\\; n \\rightarrow \\infty \\right\\}\n",
    "$$\n",
    "т.е. множество Жюлиа функции $\\;f$ - это граница множества точек $\\;z$, стремящихся к бесконечности при итерировании $f\\left( z \\right)$. Множество названо в честь французского математика Гастона Жюлиа (1893 - 1978).\n",
    "\n",
    "Рассмотрим функцию $\\; f:\\mathbb{C} \\rightarrow \\mathbb{C}$ такую, что $f\\left( z \\right)=z^2 + c$, где $c \\in \\mathbb{C}$. Итерационный процесс выглядит следующим образом\n",
    "$$\n",
    "z_{n} = f\\left( z_{n-1} \\right) = . . . = f^{n}\\left( z_{0} \\right)\n",
    "$$\n",
    "Такая динамическая система ведет себя хатически практичеси при любых $c$,т.е. две близкие начальные точки $z_{0} \\; и \\; z_{0}+\\varepsilon $ дают сильно расходящиеся траектории при больших $n$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:58:21.003337Z",
     "start_time": "2021-04-02T15:58:20.424987Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import ipyparallel\n",
    "import numpy as np\n",
    "import sys\n",
    "#sys.path.append('/home/lsauser/work_spaces/lsa-2019/Examples/')\n",
    "#import LSA_NBody as NB\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:39:55.086672Z",
     "start_time": "2021-04-02T15:39:55.074957Z"
    }
   },
   "outputs": [],
   "source": [
    "#os.system(\"ipcontroller --profile=mpi --ip=* start &\")\n",
    "#os.system(\"ipcluster start --n=4 --profile=default &\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:58:32.961551Z",
     "start_time": "2021-04-02T15:58:32.881462Z"
    }
   },
   "outputs": [],
   "source": [
    "# Создаем объект доступа к запущенному кластеру\n",
    "client = ipyparallel.Client() # Default-Cluster\n",
    "#client = ipyparallel.Client(profile='mpi') # MPI-Cluster\n",
    "#client = ipyparallel.Client(profile='polus') # Polus\n",
    "# Создаем объект доступа и управления параллельными исполнителями\n",
    "dview = client[:]\n",
    "# Указываем необходимость синхронизации\n",
    "dview.block = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:58:39.070423Z",
     "start_time": "2021-04-02T15:58:39.066513Z"
    }
   },
   "outputs": [],
   "source": [
    "# Исследуемая функция\n",
    "def f(z, c=-0.065+0.66j):\n",
    "    return z**2 + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:58:40.887310Z",
     "start_time": "2021-04-02T15:58:40.876359Z"
    }
   },
   "outputs": [],
   "source": [
    "# Исходные данные\n",
    "# n x m - разрешение исследования плоскости\n",
    "n = 1024\n",
    "m = 1024\n",
    "# zmin, zmax - ограничения комплексной области\n",
    "zmin = -1.3 - 1j * 1.3\n",
    "zmax = 1.3 + 1j * 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:58:45.091319Z",
     "start_time": "2021-04-02T15:58:45.053184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Наборы точек на осях для построения сетки\n",
    "xs = np.linspace(zmin.real, zmax.real, n)\n",
    "ys = np.linspace(zmin.imag, zmax.imag, m)\n",
    "# Построение сетки\n",
    "X, Y = np.meshgrid(xs, ys)\n",
    "Z = X + 1j * Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение множества Жюлиа в \"чистом\" виде весьма затруднительно практически, поэтому мы будем искусственно обрывать итерации по достижении некоторой границы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:58:49.116252Z",
     "start_time": "2021-04-02T15:58:49.101247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Функция построения множества Жюлиа функции f(z)\n",
    "def julia(f, Z, tmax=512):\n",
    "    J = np.ones(Z.shape) * tmax\n",
    "    for t in range(tmax):\n",
    "        mask = np.abs(Z) <= 2.\n",
    "        Z[ mask] = f(Z[mask])\n",
    "        J[~mask] -= 1\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:47:29.880748Z",
     "start_time": "2021-04-02T15:47:10.624583Z"
    }
   },
   "outputs": [],
   "source": [
    "ZZ = np.copy(Z)\n",
    "%time J = julia(f, ZZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:45:48.863722Z",
     "start_time": "2021-04-02T15:45:47.503408Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16,16])\n",
    "plt.imshow(J)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем пакет **NumPy** и необходимые функции для каждого параллельного испольнителя, используя магическую команду **jupyter** **``%%px``**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:59:27.841167Z",
     "start_time": "2021-04-02T15:59:27.661436Z"
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def f(z, c=-0.065+0.66j):\n",
    "    return z**2 + c\n",
    "\n",
    "def julia_parallel(f,Z,tmax=512):\n",
    "    J = np.ones(Z.shape) * tmax\n",
    "    for t in range(tmax):\n",
    "        mask = np.abs(Z) <= 2.\n",
    "        Z[mask] = f(Z[mask])\n",
    "        J[~mask] -= 1\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:50:38.890082Z",
     "start_time": "2021-04-02T15:50:38.807718Z"
    }
   },
   "outputs": [],
   "source": [
    "# распределяем набор данных Z по всем параллельным испольнителям\n",
    "dview.scatter('Z', Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:51:13.154915Z",
     "start_time": "2021-04-02T15:51:13.087278Z"
    }
   },
   "outputs": [],
   "source": [
    "# На каждом параллельном исполнителе создаем копию набора данных\n",
    "%px ZZ = np.copy(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:52:03.337962Z",
     "start_time": "2021-04-02T15:51:54.200871Z"
    }
   },
   "outputs": [],
   "source": [
    "# На каждом параллельном исполнителе строим множество Жюлиа функции f(z) для участка плоскости\n",
    "%px J = julia_parallel(f,ZZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:52:52.754850Z",
     "start_time": "2021-04-02T15:52:52.663676Z"
    }
   },
   "outputs": [],
   "source": [
    "# Собираем результат с каждого параллельного исполнителя\n",
    "PJ = dview.gather('J')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:53:20.215883Z",
     "start_time": "2021-04-02T15:53:19.120950Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16,16])\n",
    "plt.imshow(PJ)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы провели построение множества Жюлиа двумя способами: последовательным и параллельным. Давайте сравним время исполнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:55:22.694436Z",
     "start_time": "2021-04-02T15:55:22.665464Z"
    }
   },
   "outputs": [],
   "source": [
    "Z = X + 1j * Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:55:57.784410Z",
     "start_time": "2021-04-02T15:55:41.116916Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "J = julia(f, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:56:10.981739Z",
     "start_time": "2021-04-02T15:56:10.954199Z"
    }
   },
   "outputs": [],
   "source": [
    "Z = X + 1j * Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T15:59:50.742821Z",
     "start_time": "2021-04-02T15:59:41.412062Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "dview.scatter('Z', Z)\n",
    "%px ZZ = np.copy(Z)\n",
    "%px J = julia_parallel(f,ZZ)\n",
    "PJ = dview.gather('J')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример № 2 : Перемножение квадратных матриц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:01:56.899763Z",
     "start_time": "2021-04-02T16:01:56.251422Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import ipyparallel\n",
    "import numpy as np\n",
    "import os\n",
    "#import LSA_NBody as NB\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:01:57.975538Z",
     "start_time": "2021-04-02T16:01:57.970595Z"
    }
   },
   "outputs": [],
   "source": [
    "# настройка отображения чисел и массивов\n",
    "np.set_printoptions(precision=2)\n",
    "#np.set_printoptions(threshold=np.nan, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:02:20.443145Z",
     "start_time": "2021-04-02T16:02:20.358229Z"
    }
   },
   "outputs": [],
   "source": [
    "# Создаем объект доступа к запущенному кластеру\n",
    "client = ipyparallel.Client() # Default-Cluster\n",
    "#client = ipyparallel.Client(profile='mpi') # MPI-Cluster\n",
    "# Создаем объект доступа и управления параллельными исполнителями\n",
    "dview = client[:]\n",
    "# Указываем необходимость синхронизации\n",
    "dview.block = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:02:41.072201Z",
     "start_time": "2021-04-02T16:02:41.061083Z"
    }
   },
   "outputs": [],
   "source": [
    "def MatMul(a,b):\n",
    "    c = np.zeros_like(a)\n",
    "    for i in range(np.size(a,axis=0)):\n",
    "        for j in range(np.size(a,axis=0)):\n",
    "                   for k in range(np.size(a,axis=0)):\n",
    "                            c[i,j]+=a[i,k]*b[k,j]\n",
    "                            \n",
    "    return c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:03:16.490797Z",
     "start_time": "2021-04-02T16:03:16.483391Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "a = np.matrix(np.random.random((N,N)))\n",
    "b = np.matrix(np.random.random((N,N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:03:42.258167Z",
     "start_time": "2021-04-02T16:03:42.251039Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c = a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:05:29.505423Z",
     "start_time": "2021-04-02T16:05:29.501898Z"
    }
   },
   "outputs": [],
   "source": [
    "c = a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:04:02.644529Z",
     "start_time": "2021-04-02T16:04:02.559057Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c1 = MatMul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:05:41.716326Z",
     "start_time": "2021-04-02T16:05:41.633375Z"
    }
   },
   "outputs": [],
   "source": [
    "c1 = MatMul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:05:45.172996Z",
     "start_time": "2021-04-02T16:05:45.159764Z"
    }
   },
   "outputs": [],
   "source": [
    "np.allclose(c,c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:06:34.814456Z",
     "start_time": "2021-04-02T16:06:34.666295Z"
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def MatMul(a,b):\n",
    "    c = np.zeros_like(a)\n",
    "    for i in range(np.size(a,axis=0)):\n",
    "        for j in range(np.size(a,axis=0)):\n",
    "                   for k in range(np.size(a,axis=0)):\n",
    "                            c[i,j]+=a[i,k]*b[k,j]\n",
    "                            \n",
    "    return c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:06:41.253837Z",
     "start_time": "2021-04-02T16:06:41.226398Z"
    }
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:07:05.811722Z",
     "start_time": "2021-04-02T16:07:05.806447Z"
    }
   },
   "outputs": [],
   "source": [
    "print (client.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:08:39.329129Z",
     "start_time": "2021-04-02T16:08:39.032197Z"
    }
   },
   "outputs": [],
   "source": [
    "dview.scatter('id', client.ids, flatten=True)\n",
    "dview.scatter('a', a)\n",
    "dview.scatter('b', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:08:53.800841Z",
     "start_time": "2021-04-02T16:08:53.683338Z"
    }
   },
   "outputs": [],
   "source": [
    "%px c2 = MatMul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:09:24.703399Z",
     "start_time": "2021-04-02T16:09:24.590536Z"
    }
   },
   "outputs": [],
   "source": [
    "c2 = dview.gather(\"c2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:09:43.018819Z",
     "start_time": "2021-04-02T16:09:43.011783Z"
    }
   },
   "outputs": [],
   "source": [
    "np.allclose(c,c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:09:54.081516Z",
     "start_time": "2021-04-02T16:09:54.049364Z"
    }
   },
   "outputs": [],
   "source": [
    "c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получено неверное решение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import ipyparallel\n",
    "import numpy as np\n",
    "import os\n",
    "#import LSA_NBody as NB\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# настройка отображения чисел и массивов\n",
    "np.set_printoptions(precision=2)\n",
    "#np.set_printoptions(threshold=np.nan, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем объект доступа к запущенному кластеру\n",
    "client = ipyparallel.Client() # Default-Cluster\n",
    "#client = ipyparallel.Client(profile='mpi') # MPI-Cluster\n",
    "# Создаем объект доступа и управления параллельными исполнителями\n",
    "dview = client[:]\n",
    "# Указываем необходимость синхронизации\n",
    "dview.block = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:14:16.403984Z",
     "start_time": "2021-04-02T16:14:16.395428Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "a = np.matrix(np.random.random((N,N)))\n",
    "b = np.matrix(np.random.random((N,N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:14:30.426282Z",
     "start_time": "2021-04-02T16:14:30.421971Z"
    }
   },
   "outputs": [],
   "source": [
    "c = a*b\n",
    "#c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:15:15.195883Z",
     "start_time": "2021-04-02T16:15:14.893804Z"
    }
   },
   "outputs": [],
   "source": [
    "dview.scatter('id', client.ids, flatten=True)\n",
    "dview.scatter('a', a)\n",
    "dview.push({'b':b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:21:39.534478Z",
     "start_time": "2021-04-02T16:21:39.419473Z"
    }
   },
   "outputs": [],
   "source": [
    "dview.pull('b')[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:15:59.152919Z",
     "start_time": "2021-04-02T16:15:59.035710Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%px\n",
    "\n",
    "#print id\n",
    "c1 = a*b\n",
    "#print c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:16:20.456539Z",
     "start_time": "2021-04-02T16:16:20.351321Z"
    }
   },
   "outputs": [],
   "source": [
    "c1 = dview.gather('c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:16:27.386531Z",
     "start_time": "2021-04-02T16:16:27.379493Z"
    }
   },
   "outputs": [],
   "source": [
    "np.allclose(c1,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"text-align: center; font-size: 24pt; font-weight: bold; color: rgb(0, 0, 190); padding: 4px; margin-top: 3%; line-height: 150%;\">Изменение работы Python средствами JIT-компилятора NUMBA, пакета для работы с GPU PyCUDA и др.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/numba.svg\" width=\"70%\" /></center>\n",
    "    \n",
    "<center><b>Numba создает быстрый Python код</b></center>\n",
    "\n",
    "**Numba** - это **JIT**-компилятор с открытым исходным кодом, который преобразует подмножество кода Python и NumPy в \"быстрый\" машинный код.\n",
    "\n",
    "- **Компилятор функций:** **Numba** компилирует функции **Python**, а **не целые** приложения, и **не части** функций. **Numba** не заменяет интерпретатор **Python**, а является еще одним модулем **Python**, который преобразует функции, в функции работающие (обычно) быстрее.\n",
    "- **Специализация типов:** **Numba** ускоряет код функций, создавая специализированную реализацию для конкретных типов данных. Функции **Python** предназначены для работы с универсальными типами данных, что делает их очень гибкими, но также очень медленными. На практике вы вызываете только функцию с небольшим количеством типов аргументов, поэтому **Numba** будет генерировать быструю реализацию для каждого набора типов.\n",
    "- **just-in-time:** **Numba** переводит функции при их первом вызове. Это гарантирует, что компилятор знает, какие типы аргументов вы будете использовать. Это также позволяет использовать **Numba** интерактивно в ноутбуке **Jupyter** так же, как и традиционный код.\n",
    "- **numerically-focused:** В настоящее время **Numba** ориентирована на числовые типы данных, такие как **``int``**, **``float``** и **``complex``**. Существует очень ограниченная поддержка обработки строк, и многие примеры использования строк не будут хорошо работать, например, на графическом процессоре. Чтобы получить наилучшие результаты с **Numba**, необходимо использовать массивы **NumPy**.\n",
    "\n",
    "Компилятор **Numba** обычно активируется путем применения декоратора к функции **Python**. Декораторы - это функции, которые преобразуют код функций **Python**. Например, **``@jit``** декоратор компиляции кода для **CPU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T13:43:34.086510Z",
     "start_time": "2021-04-09T13:43:31.472460Z"
    }
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import math\n",
    "\n",
    "@jit\n",
    "def hypot(x, y):\n",
    "    x = abs(x);\n",
    "    y = abs(y);\n",
    "    t = min(x, y);\n",
    "    x = max(x, y);\n",
    "    t = t / x;\n",
    "    return x * math.sqrt(1+t*t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T13:44:03.828442Z",
     "start_time": "2021-04-09T13:44:02.483713Z"
    }
   },
   "outputs": [],
   "source": [
    "hypot(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первый раз, когда мы вызываем **hypot**, компилятор запускается и компилирует реализацию машинного кода для входов **``float``**. **Numba** также сохраняет исходную реализацию функции **Python** в атрибуте **``.py_func``**, поэтому мы можем вызвать исходный код **Python**, чтобы убедиться, что мы получим тот же ответ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T13:45:30.053871Z",
     "start_time": "2021-04-09T13:45:30.044927Z"
    }
   },
   "outputs": [],
   "source": [
    "hypot.py_func(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим ускорям ли мы исполнение кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T13:46:11.257498Z",
     "start_time": "2021-04-09T13:46:09.734691Z"
    }
   },
   "outputs": [],
   "source": [
    "%timeit hypot.py_func(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T13:47:05.276343Z",
     "start_time": "2021-04-09T13:47:01.275511Z"
    }
   },
   "outputs": [],
   "source": [
    "%timeit hypot(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Магическая команда **``%timeit``** многократно выполняет оператор, чтобы получить точную оценку времени выполнения. Он также возвращает наилучшее время по умолчанию, что полезно для уменьшения вероятности того, что случайные фоновые события повлияют на измерение.  Подход ***наилучший из 3*** также гарантирует, что время компиляции при первом вызове не искажает результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T13:47:23.338595Z",
     "start_time": "2021-04-09T13:47:20.723337Z"
    }
   },
   "outputs": [],
   "source": [
    "%timeit math.hypot(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypot.inspect_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Обратите внимание***, что имена типов **Numba** интерпретирует имена типов **NumPy**, поэтому **float Python** - это **float64** (т.е. «двойной точности» на других языках). Иногда просмотр типов данных может быть важным в коде для **GPU**, поскольку производительность вычислений **float32** и **float64** будет сильно отличаться на устройствах **CUDA**. Случайное повышение может значительно замедлить работу функции.\n",
    "\n",
    "**Numba** не может скомпилировать весь код **Python**. Некоторые функции не имеют соответствующего преобразования в **Numba**, и некоторые типы **Python** не могут быть эффективно скомпилированы вообще (пока). Например, **Numba** не поддерживает словари.\n",
    "\n",
    "## Создание ufunc для GPU\n",
    "У **Numba** есть возможность создавать скомпилированные **ufunc**. Генерация **ufunc**, которая использует **CUDA**, требует предоставления явной сигнатуры типа и установки целевого атрибута."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T13:53:43.562165Z",
     "start_time": "2021-04-09T13:53:42.923934Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import vectorize\n",
    "\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([10, 20, 30, 40])\n",
    "\n",
    "@vectorize(['int64(int64, int64)'], target='cuda')\n",
    "def add_ufunc(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T13:53:50.122354Z",
     "start_time": "2021-04-09T13:53:50.091766Z"
    }
   },
   "outputs": [],
   "source": [
    "np.add(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T13:55:31.109234Z",
     "start_time": "2021-04-09T13:55:31.101851Z"
    }
   },
   "outputs": [],
   "source": [
    "c = np.arange(4*4).reshape((4,4))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T13:56:25.933602Z",
     "start_time": "2021-04-09T13:56:25.902375Z"
    }
   },
   "outputs": [],
   "source": [
    "b_col = b[:, np.newaxis]\n",
    "b_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T13:56:49.750296Z",
     "start_time": "2021-04-09T13:56:49.741679Z"
    }
   },
   "outputs": [],
   "source": [
    "np.add(b_col, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T13:57:27.971214Z",
     "start_time": "2021-04-09T13:57:27.567375Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('a+b:\\n', add_ufunc(a, b))\n",
    "print (\"\\n\")\n",
    "print ('b_col + c:\\n', add_ufunc(b_col, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:00:59.521754Z",
     "start_time": "2021-04-09T14:00:59.372882Z"
    }
   },
   "outputs": [],
   "source": [
    "@vectorize(['int64(int64, int64)'], target='cpu')\n",
    "def add_ufunc_cpu(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:04:28.103340Z",
     "start_time": "2021-04-09T14:04:28.094209Z"
    }
   },
   "outputs": [],
   "source": [
    "print ('a+b:\\n', add_ufunc_cpu(a, b))\n",
    "print (\"\\n\")\n",
    "print ('b_col + c:\\n', add_ufunc_cpu(b_col, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пакет **Numba** автоматически:\n",
    "- Скомпилировап ядро **CUDA** для выполнения **ufunc** операции параллельно по всем входным элементам.\n",
    "- Выделил память **GPU** для входов и выхода.\n",
    "- Скопировал входные данные на графический процессор.\n",
    "- Выполнил ядро **CUDA** с учетом размеров ввода.\n",
    "- Скопировал результат с **GPU** на **CPU**.\n",
    "- Вернул результат в виде массива **NumPy** на хосте.\n",
    "\n",
    "Это очень удобно для тестирования, но копирование данных между процессором и графическим процессором может быть медленным и ухудшать производительность. \n",
    "\n",
    "Как быстро наш простой пример исполняется на графическом процессоре? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.add(b_col, c)   # NumPy на CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_ufunc(b_col, c) # Numba на GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хуже, чем на **CPU** !!! Потому, что: объем данных слишком мал, алгоритм слишком прост, используемый тип данных неоправдано велик и данные копируются на/с **GPU**.\n",
    "\n",
    "**Важно**, что для создания кода на **CUDA** необходимо использовать скалярные функции из модуля **math**, а не из **NumPy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:07:44.848506Z",
     "start_time": "2021-04-09T14:07:44.387135Z"
    }
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "SQRT_2PI = np.float32((2*math.pi)**0.5)  \n",
    "\n",
    "@vectorize(['float32(float32, float32, float32)'], target='cuda')\n",
    "def gaussian_pdf(x, mean, sigma):\n",
    "    \"\"\"Плотность вероятности распределения Гаусса\"\"\"\n",
    "    return math.exp(-0.5 * ((x - mean) / sigma)**2) / (sigma * SQRT_2PI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:09:16.846112Z",
     "start_time": "2021-04-09T14:09:16.791350Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.uniform(-3, 3, size=10000).astype(np.float32)\n",
    "mean = np.float32(0.0)\n",
    "sigma = np.float32(1.0)\n",
    "\n",
    "gaussian_pdf(x[0], 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:13:25.304233Z",
     "start_time": "2021-04-09T14:13:24.878033Z"
    }
   },
   "outputs": [],
   "source": [
    "@vectorize(['float32(float32, float32, float32)'], target='cpu')\n",
    "def gaussian_pdf_cpu(x, mean, sigma):\n",
    "    \"\"\"Плотность вероятности распределения Гаусса\"\"\"\n",
    "    return math.exp(-0.5 * ((x - mean) / sigma)**2) / (sigma * SQRT_2PI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:15:29.115648Z",
     "start_time": "2021-04-09T14:15:29.093785Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.uniform(-3, 3, size=10000).astype(np.float32)\n",
    "mean = np.float32(0.0)\n",
    "sigma = np.float32(1.0)\n",
    "\n",
    "gaussian_pdf_cpu(x[0], mean, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:17:22.152259Z",
     "start_time": "2021-04-09T14:17:19.761090Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.stats \n",
    "norm_pdf = scipy.stats.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:17:52.033546Z",
     "start_time": "2021-04-09T14:17:39.758536Z"
    }
   },
   "outputs": [],
   "source": [
    "%timeit norm_pdf.pdf(x, loc=mean, scale=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gaussian_pdf(x, mean, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:18:22.544557Z",
     "start_time": "2021-04-09T14:18:18.184259Z"
    }
   },
   "outputs": [],
   "source": [
    "%timeit gaussian_pdf_cpu(x, mean, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более сложный алгоритм, больший объем данных и лучше подобранный тип данных привели к тому, что, даже включая накладные расходы на копирование всех данных на/с **GPU**, было получено существенное ускорение выполнения кода на графическом ускорителе. **Ufuncs**, которые используют математические функции (exp, sin, cos и т.д.), применяемые к большим наборам данных, наиболее качественно ускоряются на **GPU**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции CUDA Device\n",
    "\n",
    "**Ufuncs** хороший инструмент, но если бы были только они, то весь алгоритм пришлось бы вписывать в одну функцию. Чтобы этого избежать была реализованана возможность создавать обычные функции, которые вызывается из других функций, запущенных на графическом процессоре. Этот механизм похож на функции **CUDA C**, определенные с помощью префикса **``__device__``**.\n",
    "\n",
    "**Device**-функции создаются с помощью декоратора **``numba.cuda.jit``**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:26:20.733725Z",
     "start_time": "2021-04-09T14:26:20.555122Z"
    }
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def polar_to_cartesian(rho, theta):\n",
    "    x = rho * math.cos(theta)\n",
    "    y = rho * math.sin(theta)\n",
    "    return x, y \n",
    "\n",
    "@vectorize(['float32(float32, float32, float32, float32)'], target='cuda')\n",
    "def polar_distance(rho1, theta1, rho2, theta2):\n",
    "    x1, y1 = polar_to_cartesian(rho1, theta1)\n",
    "    x2, y2 = polar_to_cartesian(rho2, theta2)\n",
    "    \n",
    "    return ((x1 - x2)**2 + (y1 - y2)**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:27:15.104531Z",
     "start_time": "2021-04-09T14:27:15.076734Z"
    }
   },
   "outputs": [],
   "source": [
    "n = 100000\n",
    "rho1 = np.random.uniform(0.5, 1.5, size=n).astype(np.float32)\n",
    "theta1 = np.random.uniform(-np.pi, np.pi, size=n).astype(np.float32)\n",
    "rho2 = np.random.uniform(0.5, 1.5, size=n).astype(np.float32)\n",
    "theta2 = np.random.uniform(-np.pi, np.pi, size=n).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:27:20.696516Z",
     "start_time": "2021-04-09T14:27:20.582020Z"
    }
   },
   "outputs": [],
   "source": [
    "polar_distance(rho1, theta1, rho2, theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit polar_distance(rho1, theta1, rho2, theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важно**, что компилятор **CUDA** агрессивно встраивает **``__device__``**-функции, поэтому для их вызовов (обычно) нет накладных расходов. Аналогично, «кортеж», возвращаемый **``polar_to_cartesian``**, фактически не создается как объект Python, а временно представлен как структура, который затем оптимизируется компилятором.\n",
    "\n",
    "## Что разрешено Python'y на GPU?\n",
    "\n",
    "По сравнению с **Numba** на **CPU** (который итак ограничен) **Numba** на **GPU** имеет еще больше ограничений. Поддерживаемый **Python** включает в себя:\n",
    "\n",
    "- `if`/`elif`/`else`\n",
    "- `while` и `for` циклы\n",
    "- основные математические операции\n",
    "- некоторые функции из модулей `math` и `cmath`\n",
    "- кортежи\n",
    "\n",
    "За более подробной информацией обращайтесь к руководству [the Numba manual](http://numba.pydata.org/numba-doc/latest/cuda/cudapysupported.html).\n",
    "\n",
    "## Управление памятью GPU\n",
    "\n",
    "Ранее мы использовали массивы **NumPy** на **CPU** в качестве входов и выходов. Если мы хотим уменьшить влияние **host-to-device/device-to-host** обмена, лучше всего скопировать данные в графический процессор и оставить их там, чтобы уменшить затраты. Кроме того, выделение памяти устройства может быть относительно медленным, поэтому однократное распределение массивов графических процессоров и их повторное наполнение данными с хоста также могут привести к улучшению производительности. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:41:04.618057Z",
     "start_time": "2021-04-09T14:41:03.805308Z"
    }
   },
   "outputs": [],
   "source": [
    "from numba import vectorize\n",
    "import numpy as np\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='cuda')\n",
    "def add_ufunc(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:41:29.665676Z",
     "start_time": "2021-04-09T14:41:29.659032Z"
    }
   },
   "outputs": [],
   "source": [
    "n = 100000\n",
    "x = np.arange(n).astype(np.float32)\n",
    "y = 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:41:50.518253Z",
     "start_time": "2021-04-09T14:41:50.131778Z"
    }
   },
   "outputs": [],
   "source": [
    "%timeit add_ufunc(x, y) # Производительность с хост-массивами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Раздел  `numba.cuda` содержит функции, которые обеспечивают копирование данных с хоста на **GPU** и возвращають **CUDA device array**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "x_device = cuda.to_device(x)\n",
    "y_device = cuda.to_device(y)\n",
    "\n",
    "print(x_device)\n",
    "print(x_device.shape)\n",
    "print(x_device.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Девайс-массивы могут передаваться в функции **CUDA** так же, как массивы **NumPy**, но без накладных расходов на копирование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_ufunc(x_device, y_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это уже значительное повышение производительности, но мы по-прежнему выделяем девайс-массив для вывода ufunc и копируем его на хост. Для улучшения ситуации мы можем создать выходной буфер с помощью функции **``numba.cuda.device_array()``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Этот способ не инициализирует память, типа np.empty()\n",
    "out_device = cuda.device_array(shape=(n,), dtype=np.float32)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем использовать специальный аргумент `out` для ufunc, чтобы указать выходной буфер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_ufunc(x_device, y_device, out=out_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы удалили шаги выделения и копирования, вычисление выполняется намного быстрее, чем раньше. Когда необходимо вернуть девайс-массив обратно на хост, мы можем использовать метод **``copy_to_host ()``**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_host = out_device.copy_to_host()\n",
    "print (out_host[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Написание ядер CUDA (CUDA Kernels)\n",
    "\n",
    "Для понимания принципов работы с **CUDA**, важно понять структуру вычислительной сетки на **GPU**.\n",
    "\n",
    "<center><img src=\"Images/grid-of-thread-blocks.png\" width=\"100%\" /></center>\n",
    "\n",
    "При работе с **GPU** мы пишем ***ядро***(kernel), которое описывает выполнение одного потока в этой иерархии. Компилятор и драйвер **CUDA** будут выполнять наше ядро, используя сетку ***thread grid***, которая делится на ***блоки*** потоков. ***Потоки*** внутри одного и того же блока могут очень легко обмениваться данными во время выполнения ядра, тогда как потоки в разных блоках обычно не должны связываться друг с другом (исключения бывают).\n",
    "\n",
    "Нахождение наилучшего размера для сетки потоков **CUDA** является сложной проблемой (и зависит как от алгоритма, так и от конкретной вычислительной способности графического процессора и размеров данных). Некоторые очень грубые рекомендации:\n",
    "\n",
    "  * размер блока должен быть кратным 32 потокам, с типовыми размерами блоков от 128 до 512 потоков на блок.\n",
    "  * размер сетки должен обеспечить, по возможности, полную загрузку графического процессор. Диапазон от 20 до 100 блоков, как правило, является хорошей отправной точкой.\n",
    "  * Накладные расходы ядра CUDA зависят от количества блоков, поэтому лучше не запускать сетку, требующую болшого ввода данных. \n",
    "\n",
    "Каждый поток отличает себя от других потоков, используя уникальные идентификаторы потока (`threadIdx`) и блока (` blockIdx`), которые могут быть многомерными.\n",
    "\n",
    "### Пример: сложение одномерных массивов\n",
    "\n",
    "Напишем функцию сложения для **1D NumPy**-массивов. Ядра **CUDA**  будем компилировать используя декоратора **``numba.cuda.jit``** (не путать с декодером **``numba.jit``** для **CPU**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def add_kernel(x, y, out):\n",
    "    # уникальный идентификатор нити внутри 1D блока\n",
    "    tx = cuda.threadIdx.x \n",
    "    # уникальный идентификатор блолка внутри 1D сетки\n",
    "    ty = cuda.blockIdx.x  \n",
    "    # число нитей в блоке\n",
    "    block_size = cuda.blockDim.x  \n",
    "    # число блоков в сетке\n",
    "    grid_size = cuda.gridDim.x    \n",
    "    \n",
    "    start = tx + ty * block_size\n",
    "    stride = block_size * grid_size\n",
    "\n",
    "    for i in range(start, x.shape[0], stride):\n",
    "        out[i] = x[i] + y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 100000\n",
    "x = np.arange(n).astype(np.float32)\n",
    "y = 2 * x\n",
    "out = np.empty_like(x)\n",
    "\n",
    "threads_per_block = 128\n",
    "blocks_per_grid = 32\n",
    "\n",
    "add_kernel[blocks_per_grid, threads_per_block](x, y, out)\n",
    "print(out[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba включает несколько [вспомогательных функций](http://numba.pydata.org/numba-doc/dev/cuda/kernels.html#absolute-positions) для упрощения вычислений смещения потоков, что позволяет писать функции несколько проще."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T15:24:41.682919Z",
     "start_time": "2021-04-09T15:24:41.662839Z"
    }
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def add_kernel(x, y, out):\n",
    "    # 1 = одномерная сетка\n",
    "    start = cuda.grid(1)      \n",
    "    # тоже\n",
    "    stride = cuda.gridsize(1) \n",
    "\n",
    "    for i in range(start, x.shape[0], stride):\n",
    "        out[i] = x[i] + y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_device = cuda.to_device(x)\n",
    "y_device = cuda.to_device(y)\n",
    "out_device = cuda.device_array_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_kernel[blocks_per_grid, threads_per_block](x, y, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_kernel[blocks_per_grid, threads_per_block](x_device, y_device, out_device); out_device.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Синхронизация в CUDA\n",
    "\n",
    "Выполнение ядра **CUDA** реализовано так, чтобы быть асинхронным относительно хост-программы. Это означает, что после запуска ядра\n",
    "(`add_kernel[blocks_per_grid, threads_per_block](x_device, y_device, out_device)`) активность немедленно возвращается к процессору и позволяет ему продолжать выполнение, в то время как графический процессор работает в фоновом режиме. Только **host<->device** копирование или явный вызов синхронизации заставят ЦП ждать, пока ядра **CUDA** не будут завершены.\n",
    "\n",
    "Когда вы передаете массивы **NumPy** с хоста в ядро **CUDA**, **Numba** должен синхронизироваться от вашего имени, но если вы передадите девайс-массивы, обработка будет продолжена. Если вы запускаете несколько ядер в последовательности без какой-либо синхронизации между ними, они будут поставлены в очередь, чтобы запускаться драйвером последовательно. Если вы хотите запускать несколько ядер на GPU параллельно (иногда это хорошая идея, но возникновение гонки ядер плохо влияет на производительность). Взгляните на [CUDA streams](http://numba.pydata.org/numba-doc/dev/cuda-reference/host.html?highlight=synchronize#stream-management).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CPU input/output arrays\n",
    "%time add_kernel[blocks_per_grid, threads_per_block](x, y, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU input/output arrays, без синхронизации внутри\n",
    "# но с принудительной синхронизацией до и после\n",
    "cuda.synchronize()\n",
    "%time add_kernel[blocks_per_grid, threads_per_block](x_device, y_device, out_device)\n",
    "cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU input/output arrays, включая явную синхронизацию внутри тайминга\n",
    "cuda.synchronize()\n",
    "%time add_kernel[blocks_per_grid, threads_per_block](x_device, y_device, out_device); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CuPy\n",
    "\n",
    "[**CuPy**](https://cupy.chainer.org/) реализует некоторый функционал **NumPy** для исполнения на **GPU**. Группа [**Preferred Networks**](https://www.preferred-networks.jp/en/) создала **CuPy** как **GPU**-backend для своей бибилиотеки глубокого обучения [**Chainer**](https://chainer.org/), но **CuPy** прекрасно работает как самостоятельная **NumPy-like GPU array** библиотека.  Если вам знаком  **NumPy**, то **CuPy** - простой способ приступить к вычислениям на **GPU**.\n",
    "\n",
    "Как и **NumPy**, **CuPy** предоставляет 3 базовые возможности:\n",
    "\n",
    "1. Создание многомерных массивов, но с размещением в памяти **GPU**.\n",
    "2. Механизм **ufunc** с похожими правилами, но исполняющимися параллельно на **GPU**.\n",
    "3. Большую библиотеку функций для работы с массивами, полностью реализованную на **CUDA**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:38:26.027638Z",
     "start_time": "2021-04-02T16:38:23.691637Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:40:05.162310Z",
     "start_time": "2021-04-02T16:38:31.194272Z"
    }
   },
   "outputs": [],
   "source": [
    "ary = cp.arange(10).reshape((2,5))\n",
    "print(repr(ary))\n",
    "print(ary.dtype)\n",
    "print(ary.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный массив находится в памяти **GPU**, по-умолчанию **GPU (device 0)**. Это можно проверить с помощью специального свойства `device`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ary.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для передачи данных с **CPU** на **GPU** необходимо использовать функцию `cp.asarray()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ary_cpu = np.arange(10)\n",
    "ary_gpu = cp.asarray(ary_cpu)\n",
    "print('cpu:', ary_cpu)\n",
    "print('gpu:', ary_gpu)\n",
    "print(ary_gpu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После завершения работы с данными на **GPU**, их можно конвертировать обратно в **NumPy**-array на **CPU** с помощью функции `cp.asnumpy()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ary_cpu_returned = cp.asnumpy(ary_gpu)\n",
    "print(repr(ary_cpu_returned))\n",
    "print(type(ary_cpu_returned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с массивами на GPU\n",
    "\n",
    "Большинство методов пакета **NumPy** поддерживаются в **CuPy**, сохраняя при этом идентичные имена и аргументы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:01:46.997032Z",
     "start_time": "2021-04-16T14:01:46.953782Z"
    }
   },
   "outputs": [],
   "source": [
    "print(ary_gpu * 2)\n",
    "print(cp.exp(-0.5 * ary_gpu**2))\n",
    "print(cp.linalg.norm(ary_gpu))\n",
    "print(cp.random.normal(loc=5, scale=2.0, size=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Документация](https://docs-cupy.chainer.org/en/stable/overview.html) **CuPy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenMP в Cython и др.\n",
    "\n",
    "**OpenMP** - это стандартный и широко используемый параллельный API, основанный на потоках, который **не** полезен непосредственно в **Python**. Причина в том, что реализация **CPython** использует **GIL**-блокировку интерпретатора, что делает невозможным одновременное выполнение нескольких потоков **Python**. Поэтому потоки не нужны для параллельных вычислений в **Python**, если только они не используются для обрамления скомпилированного кода, который выполняет **OpenMP**-распараллеливание.\n",
    "\n",
    "Это явное ограничение в интерпретаторе **Python**, и, как следствие, все распараллеливания в **Python** используют процессы (а не потоки).\n",
    "\n",
    "Тем не менее, есть способ обойти это ограничение. При вызове скомпилированного кода **GIL** освобождается, и в **Cython** можно писать **Python**-подобный код, где мы можем применять вычисления на **OpenMP**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:04:29.974254Z",
     "start_time": "2021-04-16T14:04:29.968784Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:04:42.455859Z",
     "start_time": "2021-04-16T14:04:42.449040Z"
    }
   },
   "outputs": [],
   "source": [
    "N_core = multiprocessing.cpu_count()\n",
    "print(\"This system has %d cores\" % N_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:05:33.970451Z",
     "start_time": "2021-04-16T14:05:33.963414Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:05:59.939745Z",
     "start_time": "2021-04-16T14:05:59.926008Z"
    }
   },
   "outputs": [],
   "source": [
    " %reload_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:10:05.475382Z",
     "start_time": "2021-04-16T14:10:03.817586Z"
    }
   },
   "outputs": [],
   "source": [
    "%%cython -f -c-fopenmp --link-args=-fopenmp -c-g\n",
    "\n",
    "cimport cython\n",
    "cimport numpy\n",
    "from cython.parallel import prange, parallel\n",
    "cimport openmp\n",
    "\n",
    "def cy_openmp_test():\n",
    "\n",
    "    cdef int n, N\n",
    "\n",
    "    # release GIL so that we can use OpenMP\n",
    "    with nogil, parallel():\n",
    "        N = openmp.omp_get_num_threads()\n",
    "        n = openmp.omp_get_thread_num()\n",
    "        with gil:\n",
    "            print(\"Number of threads %d: thread number %d \\n\" % (N, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:10:44.236609Z",
     "start_time": "2021-04-16T14:10:44.213991Z"
    }
   },
   "outputs": [],
   "source": [
    "cy_openmp_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T13:10:58.348122Z",
     "start_time": "2021-04-16T13:10:58.328368Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:13:48.984710Z",
     "start_time": "2021-04-16T14:13:48.959927Z"
    }
   },
   "outputs": [],
   "source": [
    "%%file fib3.f\n",
    "\n",
    "C FILE: FIB3.F\n",
    "      SUBROUTINE FIB(A,N)\n",
    "C\n",
    "C     CALCULATE FIRST N FIBONACCI NUMBERS\n",
    "C\n",
    "      INTEGER N\n",
    "      REAL*8 A(N)\n",
    "Cf2py intent(in) n\n",
    "Cf2py intent(out) a\n",
    "Cf2py depend(n) a\n",
    "      DO I=1,N\n",
    "         IF (I.EQ.1) THEN\n",
    "            A(I) = 0.0D0\n",
    "         ELSEIF (I.EQ.2) THEN\n",
    "            A(I) = 1.0D0\n",
    "         ELSE \n",
    "            A(I) = A(I-1) + A(I-2)\n",
    "         ENDIF\n",
    "      ENDDO\n",
    "      END\n",
    "C END FILE FIB3.F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:16:42.602253Z",
     "start_time": "2021-04-16T14:16:40.540144Z"
    }
   },
   "outputs": [],
   "source": [
    "!f2py --fcompiler=gfortran -c -m fib3 fib3.f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:17:50.567358Z",
     "start_time": "2021-04-16T14:17:50.562866Z"
    }
   },
   "outputs": [],
   "source": [
    "import fib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:18:37.496581Z",
     "start_time": "2021-04-16T14:18:37.480582Z"
    }
   },
   "outputs": [],
   "source": [
    "fib3.fib(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Применение пакета PyCUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:51:56.885721Z",
     "start_time": "2021-04-02T16:51:56.800298Z"
    }
   },
   "outputs": [],
   "source": [
    "import pycuda.gpuarray as gpuarray\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:51:59.038977Z",
     "start_time": "2021-04-02T16:51:58.548837Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_gpu = gpuarray.to_gpu(numpy.random.randn(5,5).astype(numpy.float32))\n",
    "a_doubled = (2*a_gpu).get()\n",
    "print (\"ORIGINAL MATRIX\")\n",
    "print (a_gpu)\n",
    "print (\"DOUBLED MATRIX AFTER PyCUDA EXECUTION USING GPUARRAY CALL\")\n",
    "print (a_doubled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.gpuarray as gpuarray\n",
    "import pycuda.autoinit\n",
    "import numpy\n",
    "from pycuda.curandom import rand as curand\n",
    "from pycuda.elementwise import ElementwiseKernel\n",
    "import numpy.linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector_a = curand((50,))\n",
    "input_vector_b = curand((50,))\n",
    "mult_coefficient_a = 2\n",
    "mult_coefficient_b = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_combination = ElementwiseKernel(\n",
    "        \"float a, float *x, float b, float *y, float *c\",\n",
    "        \"c[i] = a*x[i] + b*y[i]\",\n",
    "        \"linear_combination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_combination_result = gpuarray.empty_like(input_vector_a)\n",
    "linear_combination(mult_coefficient_a, input_vector_a,\\\n",
    "                   mult_coefficient_b, input_vector_b,\\\n",
    "                   linear_combination_result)\n",
    "\n",
    "\n",
    "print (\"INPUT VECTOR A =\")\n",
    "print (input_vector_a)\n",
    "\n",
    "print (\"INPUT VECTOR B = \")\n",
    "print (input_vector_b)\n",
    "\n",
    "print (\"RESULTING VECTOR C = \")\n",
    "print (linear_combination_result)\n",
    "\n",
    "print (\"CHECKING THE RESULT EVALUATING THE DIFFERENCE VECTOR BETWEEN C AND THE LINEAR COMBINATION OF A AND B\")\n",
    "print (\"C - (%sA + %sB) = \"%(mult_coefficient_a,mult_coefficient_b))\n",
    "print (linear_combination_result - (mult_coefficient_a*input_vector_a\\\n",
    "                                    + mult_coefficient_b*input_vector_b))\n",
    "assert (la.norm((linear_combination_result - \\\n",
    "                (mult_coefficient_a*input_vector_a +\\\n",
    "                 mult_coefficient_b*input_vector_b)).get()) < 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.gpuarray as gpuarray\n",
    "import pycuda.autoinit\n",
    "import numpy\n",
    "from pycuda.reduction import ReductionKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_length = 400\n",
    "\n",
    "input_vector_a = gpuarray.arange(vector_length, dtype=numpy.int)\n",
    "input_vector_b = gpuarray.arange(vector_length, dtype=numpy.int)\n",
    "dot_product = ReductionKernel(numpy.int,\n",
    "                       arguments=\"int *x, int *y\",\n",
    "                       map_expr=\"x[i]*y[i]\",\n",
    "                       reduce_expr=\"a+b\", neutral=\"0\")\n",
    "\n",
    "dot_product = dot_product(input_vector_a, input_vector_b).get()\n",
    "\n",
    "print(\"INPUT MATRIX A\")\n",
    "print input_vector_a\n",
    "\n",
    "print(\"INPUT MATRIX B\")\n",
    "print input_vector_b\n",
    "\n",
    "print(\"RESULT DOT PRODUCT OF A * B\")\n",
    "print dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = numpy.random.randn(5,5)\n",
    "a = a.astype(numpy.float32)\n",
    "\n",
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void doubles_matrix(float *a)\n",
    "  {\n",
    "    int idx = threadIdx.x + threadIdx.y*4;\n",
    "    a[idx] *= 2;\n",
    "  }\n",
    "  \"\"\")\n",
    "\n",
    "func = mod.get_function(\"doubles_matrix\")\n",
    "func(a_gpu, block=(5,5,1))\n",
    "\n",
    "a_doubled = numpy.empty_like(a)\n",
    "cuda.memcpy_dtoh(a_doubled, a_gpu)\n",
    "print (\"ORIGINAL MATRIX\")\n",
    "print a\n",
    "print (\"DOUBLED MATRIX AFTER PyCUDA EXECUTION\")\n",
    "print a_doubled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pycuda import driver, compiler, gpuarray, tools\n",
    "\n",
    "# -- initialize the device\n",
    "import pycuda.autoinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_code_template = \"\"\"\n",
    "__global__ void MatrixMulKernel(float *a, float *b, float *c)\n",
    "{\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "    float Pvalue = 0;\n",
    "    for (int k = 0; k < %(MATRIX_SIZE)s; ++k) {\n",
    "        float Aelement = a[ty * %(MATRIX_SIZE)s + k];\n",
    "        float Belement = b[k * %(MATRIX_SIZE)s + tx];\n",
    "        Pvalue += Aelement * Belement;\n",
    "    }\n",
    "\n",
    "    c[ty * %(MATRIX_SIZE)s + tx] = Pvalue;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATRIX_SIZE = 5\n",
    "\n",
    "a_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32)\n",
    "b_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32)\n",
    "\n",
    "c_cpu = np.dot(a_cpu, b_cpu)\n",
    "\n",
    "a_gpu = gpuarray.to_gpu(a_cpu) \n",
    "b_gpu = gpuarray.to_gpu(b_cpu)\n",
    "\n",
    "c_gpu = gpuarray.empty((MATRIX_SIZE, MATRIX_SIZE), np.float32)\n",
    "\n",
    "kernel_code = kernel_code_template % {\n",
    "    'MATRIX_SIZE': MATRIX_SIZE \n",
    "    }\n",
    "\n",
    "mod = compiler.SourceModule(kernel_code)\n",
    "\n",
    "matrixmul = mod.get_function(\"MatrixMulKernel\")\n",
    "\n",
    "matrixmul(\n",
    "    a_gpu, b_gpu, \n",
    "    c_gpu, \n",
    "    block = (MATRIX_SIZE, MATRIX_SIZE, 1),\n",
    "    )\n",
    "\n",
    "# print the results\n",
    "print \"-\" * 80\n",
    "print \"Matrix A (GPU):\"\n",
    "print a_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Matrix B (GPU):\"\n",
    "print b_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Matrix C (GPU):\"\n",
    "print c_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"CPU-GPU difference:\"\n",
    "print c_cpu - c_gpu.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(c_cpu, c_gpu.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"text-align: center; font-size: 48pt; font-weight: bold; color: rgb(0, 0, 190); padding: 4px; margin-top: 3%; line-height: 150%;\">JiT-компилятор Compyle:</p>\n",
    "<p style=\"text-align: center; font-size: 36pt; font-weight: bold; color: rgb(0, 0, 190); padding: 10px; margin-top: 3%; line-height: 150%;\">Cython, OpenMP, CUDA и OpenCL</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comрyle** позволяет пользователям выполнять скрипты, реализующие HPC алгоритмы на различных платформах. При этом используется несколько ограниченное подмножество Python. Для многоядерных архитектур поддерживается технология **OpenMP** с использованием **Cython**. Для проведения вычислений на **GPU** в ComPyle реализована поддержка технологий **OpenCL** и **CUDA**, через пакеты  **РуOpenCL** и **РуCUDA**.\n",
    "\n",
    "В начале реализуется код, с использованием ограниченного синтаксиса языка Python, затем этот код автоматически переносится на требуемую платформу HPC, компилируется и выполняется для работы либо на одном ядре ЦП, либо на нескольких ядрах ЦП, либо на графическом процессоре. **Comрyle** производит преобразование из исходного кода Python в исходныйкод С, что делает его эффективным инструментом для написания библиотек HPC. В настоящий момент некоторые пакеты прикладных программ, например, PySPH, используют **Comрyle** как встроенный инструмент ускорения вычислительного кода.\n",
    "\n",
    "**Comрyle** накладывает следующие ограничения на язык Python:\n",
    "\n",
    " - Функции с C-синтаксисом.\n",
    " - Аргументы функции должны быть объявлены с использованием аннотации типа, декоратора или аргументов по умолчанию.\n",
    " - Никаких структур данных Python, то есть списков, кортежей или словарей.\n",
    " - Поддерживаются непрерывные одномерные массивы Numpy.\n",
    " - Внутри этих функций не допускается выделение памяти.\n",
    " - В OpenCL рекурсия не поддерживается.\n",
    " - Все вызовы функций не должны использовать имена, разделенные точками, т.е. не math.sin, а просто sin. \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример № 1 : Применение Elementwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:44:22.756085Z",
     "start_time": "2021-04-16T14:44:21.613829Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import compyle\n",
    "from compyle.api import Elementwise, annotate, wrap, get_config\n",
    "import numpy as np\n",
    "import os, sys, glob\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:46:05.275834Z",
     "start_time": "2021-04-16T14:46:05.269161Z"
    }
   },
   "outputs": [],
   "source": [
    "@annotate(i='int', doublep='x, y, a, b')\n",
    "def axpb(i, x, y, a, b):\n",
    "    y[i] = a[i]*sin(x[i]) + b[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом коде doublep - это указатель на тип double. Как и все типы с 'p' на конце - это указатели на соответствующий тип."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:46:47.212116Z",
     "start_time": "2021-04-16T14:46:47.140293Z"
    }
   },
   "outputs": [],
   "source": [
    "# Исходные данные\n",
    "N = 1000000\n",
    "x = np.linspace(0, 1, N)\n",
    "y = np.zeros_like(x)\n",
    "a = np.random.random(N)\n",
    "b = np.random.random(N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:47:12.814952Z",
     "start_time": "2021-04-16T14:47:12.810239Z"
    }
   },
   "outputs": [],
   "source": [
    "# Использование OpenMP\n",
    "get_config().use_openmp = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:47:44.531597Z",
     "start_time": "2021-04-16T14:47:44.490095Z"
    }
   },
   "outputs": [],
   "source": [
    "# Реализация параллелизма с Cython.\n",
    "backend = 'cython'\n",
    "e = Elementwise(axpb, backend=backend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:48:36.640732Z",
     "start_time": "2021-04-16T14:48:36.598544Z"
    }
   },
   "outputs": [],
   "source": [
    "%time e(x, y, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T13:01:06.727615Z",
     "start_time": "2021-04-16T13:01:06.723932Z"
    }
   },
   "outputs": [],
   "source": [
    "# Использование OpenCL\n",
    "get_config().use_opencl = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T14:50:35.625667Z",
     "start_time": "2021-04-16T14:50:35.549166Z"
    }
   },
   "outputs": [],
   "source": [
    "# Реализация параллелизма с OpenCL.\n",
    "backend = 'opencl'\n",
    "x, y, a, b = wrap(x, y, a, b, backend=backend)\n",
    "\n",
    "e = Elementwise(axpb, backend=backend)\n",
    "e(x, y, a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time e(x, y, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример № 2: Сравнения разных реализаций параллельных вычислений в Compyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compyle.api import Elementwise, annotate, wrap, get_config\n",
    "import numpy as np\n",
    "from numpy import sin\n",
    "import time\n",
    "\n",
    "\n",
    "@annotate(i='int', doublep='x, y, a, b')\n",
    "def axpb(i, x, y, a, b):\n",
    "    y[i] = a[i]*sin(x[i]) + b[i]\n",
    "\n",
    "\n",
    "def setup(backend, openmp=False):\n",
    "    get_config().use_openmp = openmp\n",
    "    e = Elementwise(axpb, backend=backend)\n",
    "    return e\n",
    "\n",
    "\n",
    "def data(n, backend):\n",
    "    x = np.linspace(0, 1, n)\n",
    "    y = np.zeros_like(x)\n",
    "    a = x*x\n",
    "    b = np.sqrt(x + 1)\n",
    "    return wrap(x, y, a, b, backend=backend)\n",
    "\n",
    "\n",
    "def compare(m=20):\n",
    "    N = 2**np.arange(1, 26)\n",
    "    backends = [['cython', False], ['cython', True]]\n",
    "\n",
    "    try:\n",
    "        import pycuda\n",
    "        backends.append(['cuda', True])\n",
    "    except ImportError as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        import pycuda\n",
    "        backends.append(['opencl', True])\n",
    "    except ImportError as e:\n",
    "        pass\n",
    "    \n",
    "    timing = []\n",
    "    for backend in backends:\n",
    "        e = setup(*backend)\n",
    "        times = []\n",
    "        for n in N:\n",
    "            args = data(n, backend[0])\n",
    "            t = []\n",
    "            for j in range(m):\n",
    "                start = time.time()\n",
    "                e(*args)\n",
    "                secs = time.time() - start\n",
    "                t.append(secs)\n",
    "            times.append(np.average(t))\n",
    "        timing.append(times)\n",
    "\n",
    "    return N, backends, np.array(timing)\n",
    "\n",
    "\n",
    "def plot_timing(n, timing, backends):\n",
    "    from matplotlib import pyplot as plt\n",
    "    backends[1][0] = 'openmp'\n",
    "    for t, backend in zip(timing[1:], backends[1:]):\n",
    "        plt.semilogx(n, timing[0]/t, label='serial/' + backend[0], marker='+')\n",
    "    plt.grid()\n",
    "    plt.xlabel('N')\n",
    "    plt.ylabel('Speedup')\n",
    "    plt.legend()\n",
    "    plt.savefig('ComPyle_SpeedUp.png',dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "n, backends, times = compare()\n",
    "plot_timing(n, times, backends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "<center>\n",
    "<table style=\"background: transparent; text-align: center; font-size: 36pt; font-weight: bold; margin-top: -5%;\">\n",
    "<tr >\n",
    "<td><img src=\"Images/numpy_logo.png\" width=\"180\" /><b>NumPy</b></td>\n",
    "<td><center><img src=\"Images/scipy_logo.png\" width=\"180\" /><b>SciPy</b></td>\n",
    "<td><center><img src=\"Images/SymPy_logo.jpg\" width=\"180\" /></td>\n",
    "<td><center><img src=\"Images/Pythonchik_2.png\" width=\"180\" /></td>\n",
    "</tr>\n",
    "</table>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "livereveal": {
   "autolaunch": false,
   "height": 800,
   "scroll": true,
   "theme": "sky",
   "width": 1200
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
